---
title: ""
author: "Spotification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TO DO
- create updated version of unique_tracks_genres.csv to do all the data cleaning (make this code neater)
- [JENNY] Data cleaning 


# Project outline

In this project, we analyse determinants of song popularity from a dataset on Spotify tracks. 

In particular, our original dataset covers 114000 tracks. Each track has 21 audio features associated with it, ranging from artist name, popularity, duration, genre, ‘acousticness’, and tempo. All measures that cannot be measured directly such as ‘acousticness’, ‘danceability’, ‘instrumentalness’, have been normalised to a scale of 0-1.

We feel it would be interesting to see what factors affect 'popuarity', and believe it is likely to be determined by the other regressors in the data set such as ‘energy’, ‘danceability’, ‘valence’ etc. This could produce valuable models by predicting which songs people will enjoy before they’ve become popular, based on the characteristic or ‘intrinsic’ value of the song and less so about the artist names attached to it. Hence this can help with ‘song recommendation’ features. 

It is reasonable to assume each track is independent of another, given that songs are usually written based on new concepts. We can also assume they share the same probability distribution, since all songs are judged based on the same critera, all of which are normalised to the same scale. Hence it is reasonable to assune they are identically distrubuted.

Dataset source: 
https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset 

# [JENNY] Data cleaning 

- problems with original dataset
- motivation on ridding of certain aspects
- explanation of new dataset

Notes: 
- track_id unique
- track_name not unique 
(keep different versions by different singers, rid of different versions by the same singer)

# Load libraries and data

```{r}
library(tidyverse) 
library(GGally) # ggpairs()
library(corrplot) # corrplot()
library(gridExtra) # grid.arrange()
library(ggplot2) # gm_scatterplot
library(tidymodels) # initial_split()
library(glmnet) # glmnet()
```

```{r}
data_new <- read.csv("unique_tracks_genres.csv")
final_data <- select(data_new, -X, -track_id, -artists, -album_name, -track_name, -track_genre)

# convert into factors
final_data$explicit <- as.numeric(as.factor(final_data$explicit))-1 # 0 for FALSE, 1 for TRUE
final_data$mode <- as.integer(final_data$mode) # 0 for minor, 1 for major

# create new dummy showing if track has >1 genre
genres <- c("pop", "rock", "country", "jazz", "electronic", "classical", "world", "kids", "other", "rap")
final_data$two_genre <- as.numeric(rowSums(final_data[, genres]) == 2)
final_data$three_genre <- as.numeric(rowSums(final_data[, genres]) == 3)
final_data$four_genre <- as.numeric(rowSums(final_data[, genres]) == 4)
final_data$five_genre <- as.numeric(rowSums(final_data[, genres]) == 5)
```

# Exporatary analysis

### Summary and visual

```{r}
summary(final_data)
```

```{r}
col_names <-names(final_data)
for (i in seq_along(col_names)){
  hist(final_data[,i], main=paste("Histogram of", col_names[[i]]))
}
```

### Correlation plot

```{r}
final_data_cor1 <- cor(final_data)
corrplot(final_data_cor1, method="square", col = rev(colorRampPalette(c("#B40F20", "#FFFFFF", "#2E3A87"))(100)), type="lower", tl.col="black", tl.srt=60, tl.cex = 0.6)
```

### ggpairs plot 
The features selected are selected based on high absolute correlation between factors in the correlation plot.

```{r}
ggpairs(final_data, columns = c("popularity", "danceability", "loudness", "instrumentalness"), lower = list(continuous = "smooth"), upper = list(continuous = "cor"))
```

### Standardised metrics
Popularity vs: danceability, energy, speechiness, acousticness, instrumentalness, liveness, valence

```{r}
basic_plots <- function(x){
  # plot without transparency  
  plot_nt <- ggplot(final_data, aes(x = !!sym(x), y = popularity)) +
    geom_point(alpha = 0.1)
  # plot with transparency 
  plot_wt <- ggplot(final_data, aes(x = !!sym(x), y = popularity)) +
    geom_bin2d(alpha = 0.7) +
    scale_fill_gradientn(colors = c("#440154", "#30678D", "#35B778", "#FDE724", "#FFFFFF"))
  # Return both plots 
  return(list(plot_nt, plot_wt))
}

metrics <- c('danceability', 'energy', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence')
for (i in metrics) {
  plots <- basic_plots(i)
  grid.arrange(plots[[1]], plots[[2]], ncol = 2)
}
```

### Genre

```{r}
# Assign the genre name based on the dummy variables
get_genre_name <- function(x) {
  ifelse(x["two_genre"] == 1, "2_genres",
    ifelse(x["three_genre"] == 1, "3_genres",
      ifelse(x["four_genre"] == 1, "4_genres",
        ifelse(x["five_genre"] == 1, "5_genres",
          ifelse(x["rock"] == 1, "rock",
            ifelse(x["country"] == 1, "country",
              ifelse(x["jazz"] == 1, "jazz",
                ifelse(x["electronic"] == 1, "electronic",
                  ifelse(x["classical"] == 1, "classical",
                    ifelse(x["world"] == 1, "world",
                      ifelse(x["kids"] == 1, "kids",
                        ifelse(x["other"] == 1, "other",
                          ifelse(x["rap"] == 1, "rap", "pop")))))))))))))
}
# Apply the function to each row of the data frame and create a new column with the genre names
temp_data <- data.frame(final_data)
temp_data$genre_name <- apply(final_data[, -1], 1, get_genre_name)

# Create a bar plot of mean popularity by genre
mean_popularity <- tapply(temp_data$popularity, temp_data$genre_name, mean)
barplot(mean_popularity, xlab = "Genre", ylab = "Mean Popularity", col = "steelblue", main = "Mean Popularity by Genre", las = 2, cex.names = 0.8)
```

# Modelling

### Set up the Tidymodels Framework

```{r}
# Define X, y, data
X <- select(final_data, -1)
y <- final_data$popularity
data <- data.frame(y = y, X = X)

# Split data into training and test set
data_split <- initial_split(data)
data_train <- training(data_split)
data_test <- testing(data_split)

# Cross-validation for tuning the parameters
data_cv <- vfold_cv(data_train, v = 10)

# Pre-process the model
data_recipe <- data_train %>%
  recipe(y ~ .) %>%
  prep()
```

### Baseline model
Simple baseline for comparison to the more sophisticated models. Here we have chosen linear regression. 

```{r}
baseline <- lm(y ~ X.explicit + X.danceability + X.instrumentalness, data = data_train)
predictions_baseline <- predict(baseline, newdata = data_test)

# Test metrics ------------------
RMSE_baseline <- sqrt(mean((data_test$y - predictions_baseline)^2))
RSQ_baseline <- cor(data_test$y, predictions_baseline)^2

# Print the value
print("Testing: ")
cat("RMSE:", RMSE_baseline, "\n")
cat("R-squared:", RSQ_baseline, "\n")

# Training metrics

# Get summary statistics ------------------
summary_stats <- summary(baseline)
# Extract RMSE and R-squared values
RMSE_baseline_train <- sqrt(mean(summary_stats$residuals^2))
RSQ_baseline_train <- summary_stats$r.squared

print("Training: ")
cat("RMSE:", RMSE_baseline_train, "\n")
cat("R-squared:", RSQ_baseline_train, "\n")
```

### Lasso / Ridge / Elastic-net
Non-baseline model that is (relatively) interpretable.

  - Defines a linear regression model with Lasso regularization using the `linear_reg()` function from the `parsnip` package
  - `tune()` used to specify the hyperparameters `penalty` (P) and `mixture` (M)
  - `set_engine()` used to specify the modeling engine used to fit the model (here we use `glmnet`)
  - the resulting object `pen_reg_y` is a model specification object that can be further used for model training, tuning and prediction 

```{r}
# Model specification = penalised linear regression
pen_reg_y <- linear_reg(penalty = tune('P'), mixture = tune('M')) %>%
  set_engine('glmnet')

# Set up the workflow
pen_reg_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(pen_reg_y)

# Tune the parameters
fit_pen_reg <- tune_grid(pen_reg_wf,
                         #grid = data.frame(P = 2^seq(-3, 2, by = 1),
                                           #M = seq(0, 1, by = 0.2)),
                         data_cv,
                         metrics = metric_set(rmse, mae, rsq),
                         control = control_grid(save_pred = TRUE))
fit_pen_reg %>% autoplot()  # plot the result for each value of the parameters

# Select the best model with the smallest cross-validation rmse
pen_reg_best <- fit_pen_reg %>%
  select_best(metric = 'rmse') 
pen_reg_best   # print the best model

### After getting the best parameter, can now return to the normal function

# Fit the final model
pen_reg_final <- finalize_model(pen_reg_y, pen_reg_best)

# Predict on the test data with the final model
pen_reg_test <- pen_reg_wf %>%
  update_model(pen_reg_final) %>%
  last_fit(split = data_split) %>%
  collect_metrics()
pen_reg_test  # print the result
```

```{r}
P_best <- pen_reg_best[1]
M_best <- pen_reg_best[2]

glmnet_best <- glmnet(select(data_train, -1), data_train$y,
                    family = "gaussian",
                    alpha = M_best)
glmnet_lasso <- glmnet(select(data_train, -1), data_train$y,
                    family = "gaussian",
                    alpha = 1)
glmnet_ridge <- glmnet(select(data_train, -1), data_train$y,
                    family = "gaussian",
                    alpha = 0)

plot(glmnet_best, xvar = "lambda")
```

Interpretation:


Comparison to baseline model:
Predictive accuracy better. This can be seen through the lower RMSE. The R-Squared has also improved from 0.03 to 0.11. Despite this, it is still very low. This gives us reason to think perhaps the relationship is not linear. Therefore, one of the models that followed that was complex and non-linear: Random forest


### [PARK] Gradient descent (minibatch)

### [JENNY] Random forest 

# Evaluation 

