---
title: "ST310: Final Project"
author: "Spotification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TO DO
- create updated version of unique_tracks_genres.csv to do all the data cleaning (make this code neater)
- [JENNY] Data cleaning 


# 1) Executive Summary

TBC

# 2) Motivation

Have you ever wondered what makes a song popular? Does the success comes from the characteristics of the song or the artist name? In this project, we attempt to isolate any impacts from the artists from the features of the song and determine whether songs with certain features are likely to be more popular than others.

We conjecture that factors affecting 'popularity' are likely to be from other audio characteristics of a song such as ‘energy’, ‘danceability’, ‘valence’ or 'genre' etc. If a relationship exists, this could produce valuable models by predicting which songs people will enjoy before they’ve become popular, based on the ‘intrinsic’ value of the song and less so about the artist names attached to it. Hence this can help with the **song recommendation** features. 

Whether audio features and popularity are related through a linear relationship or not is also of our interest. Given that the true relationship is unknown, different models are proposed and their predictive performance are compared. We start out with a **multiple linear regression** model with a few chosen predictors as our benchmark. Then, we extend this to produce a relatively interpretable models using **penalised regression** (ridge, Lasso and elastic net). **Mini-batch gradient descent** algorithm is also implemented on linear regression to see if it gives any improvement. Finally, a **random forest** model is built on the higher-dimensional version of the dataset (details are discussed below), focusing solely on the prediction accuracy. 

It is reasonable to assume each track is independent of another, given that songs are usually written based on new concepts. We can also assume they share the same probability distribution, since all songs are judged based on the same criteria, all of which are normalised to the same scale. Hence it is reasonable to assume they are identically distributed.

# 3) Description of the Dataset

In this project, we analyse determinants of song popularity from a (dataset on Spotify tracks)[https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset]. 

In particular, our original dataset covers 114,000 tracks. Each track has 21 audio features associated with it, ranging from artist name, popularity, duration, genre, ‘acousticness’, and tempo. All 'intangible' measures that cannot be measured directly such as ‘acousticness’, ‘danceability’, ‘instrumentalness’ have been normalised to a scale of 0-1. 

The following lists 16 variables used in the analysis. 

1. **popularity**: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.
2. **duration_ms**: The track length in milliseconds
3. **explicit**: Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)
4. **danceability**: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable
5. **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale
6. **key**: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1
7. **loudness**: The overall loudness of a track in decibels (dB)
8. **mode**: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0
9. **speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks
10. **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic
11. **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content
12. **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live
13. **valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)
14. **tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration
15. **time_signature**: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.
16. **track_genre**: The genre in which the track belongs.

*Source*: [Kaggle](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset)

# 4) [JENNY] Data cleaning 

- problems with original dataset
- motivation on ridding of certain aspects
- explanation of new dataset

Notes: 
- track_id unique
- track_name not unique 
(keep different versions by different singers, rid of different versions by the same singer)

## Please correct the following
We note the importance the 'genre' of a song may have on its popularity and that a track could be many genres. Therefore, we decided to create new dummy  variables for each genre (and their combinations), specifying whether a song falls into a category or not.

# 5) Exploratary Data Analysis

## 5.1) Load Libraries and Read in the Data

We will use the `tidymodels` package to facilitate our workflow, particularly with tuning the parameters. In addition, after the optimal paramters are chosen, some models may be fitted using their own stand-alone package for further analysis. 

```{r, message = FALSE}
library(tidyverse) 
library(tidymodels) # initial_split()

library(GGally) # ggpairs()
library(corrplot) # corrplot()
library(gridExtra) # grid.arrange()
library(ggplot2) # gm_scatterplot

library(glmnet) # glmnet()
library(randomForest)  # randomForest()
```

Sine we exclude the analysis of popularity based on artist names, we first filter these irrelevant columns out. 

```{r, eval = FALSE}
data_raw <- read.csv("unique_data.csv")
data_full <- select(data_raw, -X, -track_id, -artists, -album_name, 
                    -track_name, -track_genre)

# Convert variables into numerical values 
# 0 for FALSE, 1 for TRUE
data_full$explicit <- as.numeric(as.factor(data_full$explicit))-1 

# Create new dummy showing if track has >1 genre -- do we still need this
genres <- c("country", "electronic", "jazz", "pop", "r_n_b", "rock", 
            "classical", "metal", "other", "world", "latin")
#data_full$two_genre <- as.numeric(rowSums(data_full[, genres]) == 2)
#data_full$three_genre <- as.numeric(rowSums(data_full[, genres]) == 3)
#data_full$four_genre <- as.numeric(rowSums(data_full[, genres]) == 4)
#data_full$five_genre <- as.numeric(rowSums(data_full[, genres]) == 5)

# Create a new variable counting the number of genres it has
data_full$ngenre <- as.numeric(rowSums(data_full[, genres]))

rm(data_raw)
```

Then, for a fair and accurate performance evaluation, the 'training' and 'test' sets of the data is being created.

```{r, eval = FALSE}
# Split data into training and test set
set.seed(12345)
data_split <- initial_split(data_full)
data_train <- training(data_split)
data_test <- testing(data_split)
```

```{r, echo = FALSE, eval = FALSE}
# Write csv files to store this testing data
write.csv(data_train, "training_data.csv", row.names = FALSE)
write.csv(data_test, "testing_data.csv", row.names = FALSE)
```

```{r, echo = FALSE}
data_train <- read.csv("training_data.csv", header = TRUE)
data_test <- read.csv("testing_data.csv", header = TRUE)
```

## 5.2) Exploratary Data Analysis

Before building any model, we perform an exploratory data analysis to see if there is any notable relationship that is worth exploring. 

### Summary and visual

```{r}
# Maybe choose only important ones? Eg. popularity, danceability etc.
col_names <-names(data_train)
for (i in seq_along(col_names)){
  hist(data_train[,i], main=paste("Histogram of", col_names[[i]]))
}
```

Maybe worth taking log for highly skewed variables:
duration_ms, loudness, speechiness, acousticness ?, instrumentalness ?, liveness ?

### Correlation plot

```{r}
data_cor1 <- cor(data_train)
corrplot(data_cor1, method="square", col = rev(colorRampPalette(c("#B40F20", "#FFFFFF", "#2E3A87"))(100)), type="lower", tl.col="black", tl.srt=60, tl.cex = 0.6)
```

It seems like popularity doesn't seem to be linearly correlated with any of the predictors, but there are some features that seem to be more 'important' than others. So, we select certain features with high absolute correlation with popularity and explore their associations further.

### ggpairs plot 

Even though the magnitude of correlation seems to be low, hypothesis tests confirm that they are indeed significantly different from 0. The density plot shows that a proportion of the tracks have near-zero popularity, whilst very few are super popular. The distribution of each regressor also differ with danceability being more symmetric whereas loudness and instrumentalness are highly negatively and postively skewed, respectively. With highly skewed distributions, log-transformation could help improve the performance.

```{r}
ggpairs(data_train, columns = c("popularity", "danceability", "loudness", "instrumentalness"), lower = list(continuous = "smooth"), upper = list(continuous = "cor"))
```

### Standardised metrics (Is there information to be gained here?)

Popularity vs: danceability, energy, speechiness, acousticness, instrumentalness, liveness, valence

```{r}
basic_plots <- function(x){
  # plot without transparency  
  plot_nt <- ggplot(data_train, aes(x = !!sym(x), y = popularity)) +
    geom_point(alpha = 0.1)
  # plot with transparency 
  plot_wt <- ggplot(data_train, aes(x = !!sym(x), y = popularity)) +
    geom_bin2d(alpha = 0.7) +
    scale_fill_gradientn(colors = c("#440154", "#30678D", "#35B778", "#FDE724", "#FFFFFF"))
  # Return both plots 
  return(list(plot_nt, plot_wt))
}

metrics <- c('danceability', 'energy', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence')
for (i in metrics) {
  plots <- basic_plots(i)
  grid.arrange(plots[[1]], plots[[2]], ncol = 2)
}
```

### Genre

```{r}
# Assign the genre name based on the dummy variables
get_genre_name <- function(x) {
  ifelse(x["two_genre"] == 1, "2_genres",
    ifelse(x["three_genre"] == 1, "3_genres",
      ifelse(x["four_genre"] == 1, "4_genres",
        ifelse(x["five_genre"] == 1, "5_genres",
          ifelse(x["rock"] == 1, "rock",
            ifelse(x["country"] == 1, "country",
              ifelse(x["jazz"] == 1, "jazz",
                ifelse(x["electronic"] == 1, "electronic",
                  ifelse(x["classical"] == 1, "classical",
                    ifelse(x["world"] == 1, "world",
                      ifelse(x["kids"] == 1, "kids",
                        ifelse(x["other"] == 1, "other",
                          ifelse(x["rap"] == 1, "rap", "pop")))))))))))))
}
# Apply the function to each row of the data frame and create a new column with the genre names
temp_data <- data.frame(data_train)
temp_data$genre_name <- apply(data_train[, -1], 1, get_genre_name)

# Create a bar plot of mean popularity by genre
mean_popularity <- tapply(temp_data$popularity, temp_data$genre_name, mean)
barplot(mean_popularity, xlab = "Genre", ylab = "Mean Popularity", col = "steelblue", main = "Mean Popularity by Genre", las = 2, cex.names = 0.8)
```

# 6) Modelling

## 6.1) Set up the Tidymodels Framework

```{r}
# Cross-validation for tuning the parameters
data_cv <- vfold_cv(data_train, v = 10)

# Pre-process the model
data_recipe <- data_train %>%
  recipe(popularity ~ .) %>%
  prep()
```

## 6.2) Baseline Model

We chose a multiple linear regression model with 3 parameters for a simple baseline for comparison to the more sophisticated models.

### Add why you choose these

```{r}
# Fit the baseline model
baseline <- lm(popularity ~ explicit + danceability + instrumentalness, 
               data = data_train)

# Training metrics
summary_stats <- summary(baseline)

# Extract RMSE and R-squared values
RMSE_baseline_train <- sqrt(mean(summary_stats$residuals^2))
RSQ_baseline_train <- summary_stats$r.squared

print("Training: ")
cat("RMSE:", RMSE_baseline_train, "\n")
cat("R-squared:", RSQ_baseline_train, "\n")

# Evaluate the model on the validation set
predictions_baseline <- predict(baseline, newdata = data_test)

# Validation metrics
RMSE_baseline <- sqrt(mean((data_vtest$popularity - predictions_baseline)^2))
RSQ_baseline <- cor(data_test$popularity, predictions_baseline)^2  # change this

# Print the value
print("Testing: ")
cat("RMSE:", RMSE_baseline, "\n")
cat("R-squared:", RSQ_baseline, "\n")
```

### Add interpretation of the coefficients ??
**Interpretation**:

## 6.3) Penalised Regression (Ridge / Lasso / Elastic Net)

This is a  non-baseline model that is (relatively) interpretable.

  - Defines a linear regression model with Lasso regularization using the `linear_reg()` function from the `parsnip` package
  - `tune()` used to specify the hyperparameters `penalty` (P) and `mixture` (M)
  - `set_engine()` used to specify the modeling engine used to fit the model (here we use `glmnet`)
  - the resulting object `pen_reg_y` is a model specification object that can be further used for model training, tuning and prediction 

```{r}
# Model specification for penalised linear regression
pen_reg_y <- linear_reg(penalty = tune('P'), mixture = tune('M')) %>%
  set_engine('glmnet')

# Set up the workflow
pen_reg_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(pen_reg_y)

# Tune the parameters
fit_pen_reg <- tune_grid(pen_reg_wf,
                         #grid = data.frame(P = 2^seq(-3, 2, by = 1),
                                           #M = seq(0, 1, by = 0.2)),
                         data_cv,
                         metrics = metric_set(rmse, mae, rsq),
                         control = control_grid(save_pred = TRUE))

# Plot the result for each value of the parameters
fit_pen_reg %>% autoplot()  

# Select the best model with the smallest cross-validation RMSE
pen_reg_best <- fit_pen_reg %>%
  select_best(metric = 'rmse') 
pen_reg_best   # print the best model

# Fit the final model
pen_reg_final <- finalize_model(pen_reg_y, pen_reg_best)

# Predict on the validation data with the final model
pen_reg_test <- pen_reg_wf %>%
  update_model(pen_reg_final) %>%
  last_fit(split = data_split) %>%
  collect_metrics()
pen_reg_test  # print the result
```

With the optimal parameters, we use the normal `glmnet` function to investigate which predictors are being treated as 'important' in predicting popularity. 

```{r}
# Define X, y, data for later use
ytrain <- data_train$popularity
Xtrain <- as.matrix(select(data_train, -1))  # all regressors

# Optimal parameters
P_best <- pen_reg_best[1]
M_best <- pen_reg_best[2]

# Using the best mixture (elastic net)
glmnet_best <- glmnet(Xtrain, ytrain,
                    family = "gaussian",
                    alpha = M_best)

# Pure LASSO model
glmnet_lasso <- glmnet(Xtrain, ytrain,
                    family = "gaussian",
                    alpha = 1)

# Pure ridge regression
glmnet_ridge <- glmnet(Xtrain, ytrain,
                    family = "gaussian",
                    alpha = 0)

# Coefficient paths
plot(glmnet_best, xvar = "lambda")
```

**Interpretation**:

### Add what are the important predictors ???

**Comparison to baseline model**:
The elastic net model gives better accuracy. This can be seen through the lower RMSE. The R-Squared has also improved from 0.03 to 0.11. Despite this, it is still very low. This agrees, to some extent, with the correlation plot that perhaps the relationship is non-linear. Therefore, one of the models that followed that was complex and non-linear: Random forest ?? (sentence not finished)

## 6.4) Mini-batch Gradient Descent

Although it seems that the relationship is not linear, we have another go at implementing mini-batch gradient descent on linear regression with a (pseudo-Huber loss)[https://en.wikipedia.org/wiki/Huber_loss] function. 

### Pseudo-Huber Loss Function

Instead of minimising the sum of squares in linear regression or using the $L_1$-loss function in Lasso, we use a differentiable mixture of the two. It is a convex function, especially when close to the minimum, so ensuring that we will get a global minimum rather than a local one. The parameter $\delta$ controls the steepness of the loss function. 

*Source*: (Wikipedia)[https://en.wikipedia.org/wiki/Huber_loss]

The pseudo-Huber loss function is defined as:

$$
  L_\delta(a) = \delta^2 \Big( \sqrt{1 + \Big( \frac{a}{\delta} \Big)^2} - 1 \Big)
$$
where $a$ is the residuals from linear regression.

```{r}
pseudo_huber_loss <- function(y, X, beta, delta = 1.2) {
  a <- y - X %*% beta
  loss <- delta^2 * (sqrt(1 + (a / delta)^2) - 1)
  return(sum(loss))
}
```

We illustrate the pseudo-Huber loss function for different values of $\delta$, with particular emphasis on the change of curvature from $L_1$ loss function to $L_2$ around $a = 0$.

```{r}
sim_data <- data.frame(a = seq(-4, 4, length.out = 1000),
                       L1 = numeric(1000),
                       L2 = numeric(1000),
                       delta_2 = numeric(1000),
                       delta_5 = numeric(1000),
                       delta_10 = numeric(1000))

sim_data$L1 = abs(sim_data$a)
sim_data$L2 = sim_data$a^2
i <- 4
for (delta in c(2, 5, 10)) {
  sim_data[, i] <- delta^2 * (sqrt(1 + (sim_data$a / delta)^2) - 1)
  i <- i + 1
}

ggplot(sim_data) +
  geom_line(aes(a, L1, color = "L1"), linetype = 2) +
  geom_line(aes(a, L2, color = "L2"), linetype = 2) +
  geom_line(aes(a, delta_2, color = "2")) +
  geom_line(aes(a, delta_5, color = "5")) +
  geom_line(aes(a, delta_10, color = "10")) +
  labs(title = "Pseudo-Huber VS L1 VS L2 Loss Function",
       x = "Residuals", y = "Loss", color = "Delta") +
  ylim(0, 16)
```

**Comment**: The higher the value of $\delta$, the steeper the loss function gets for residuals of greater magnitude, thus penalising them more and approximate the $L_2$ loss function more closely. Alternatively, $\delta$ can also be thought of as the point where the pseudo-Huber loss function changes from $L_1$ to $L_2$ loss. However, even if $\delta \to \infty$, there is a limit to how the loss function tends towards $L_2$. In fact, the limiting loss function is not far from that of $\delta = 10$.

Ideally, $\delta$ is another tuning parameter in the gradient descent algorithm. However, given the amount of data and the need to optimise the execution time, we decided to fix $\delta = 5$. The rationale behind this is that we don't want outliers to have as much impact as it was in $L_2$ loss function, but still want to penalise them more heavily than others. Hence, we seek a balance between $L_1$ and $L_2$ loss function, but closer to $L_1$.

### Gradient Function

Since the pseudo-Huber loss function is differentiable everywhere, we use exact differentiation in the algorithm. Note that the result is being cross-checked with numeric differentiation. It's partial derivative is given by:

$$
  \frac{\partial{L_\delta(a, \beta_j)}}{\partial{\beta_j}} = \frac{-a x_j}{\sqrt{1 + \Big( \frac{a}{\delta} \Big)^2}}
  \qquad \text{for } j = 1, 2, \ldots, p
$$

```{r}
gradient_vector <- function(y, X, beta, delta = 1.2) {
  gradient <- rep(1, ncol(X))
  
  a <- y - X %*% beta
  scale_factor <- -a / sqrt(1 + (a / delta)^2)
  for (j in 1:ncol(X)) {
    gradient[j] <- sum(scale_factor * X[,j])
  }
  return(gradient)
}
```

### Updating the Weights

For each batch of data, we then compute the gradient and update the weights (betas). The direction of the update is being controlled by the sign of the gradient. By normalising the gradient to have magnitude of 1, we control the size of the step by the learning rate. The weights are updated according to the equation below.

$$
  \beta_{i+1} = \beta_i - \alpha \frac{L_{\delta}'(a;\beta_i)} {||L_{\delta}'(a; \beta_i)||_2}
$$
where the learning rate, $\alpha$ is a tuning parameter.

```{r}
update_beta <- function(y_batch, X_batch, beta, delta = 1.2, alpha = 0.8) {
  # Calculate gradient
  gradient <- gradient_vector(y_batch, X_batch, beta, delta)
  
  l2_norm <- sqrt(sum(gradient)^2)
  
  # Update beta
  new_beta <- beta - alpha * gradient / (l2_norm + 10^-9)  # to avoid division by 0
  
  return(new_beta)
}
```

### Mini-batch Gradient Descent for 1 Iteration

We then write a function to perform mini-batch gradient descent, where a batch of data is randomly sampled and used to update the weights. We repeat this until all batches have been used. The number of batches is calculated such that each data point is in exactly 1 batch, in expectation.

```{r}
mini_batch_gd <- function(y, X, batch_size = 10000, 
                          init_beta = rep(0.1, ncol(X)),
                          delta = 1.2, alpha = 0.8) {
  # Number of batches
  num_batches <- ceiling(nrow(X)/batch_size)
  
  for (i in 1:num_batches) {
    # Randomly sample the batch with replacement
    batch_index <- sample(1:nrow(X), batch_size, replace = TRUE)
    y_batch <- y[batch_index]
    X_batch <- X[batch_index, ]
  
    # Update the beta 
    updated_beta <- update_beta(y_batch, X_batch, init_beta, 
                                delta = delta, alpha = alpha)
  
    init_beta <- updated_beta
  }

  return(init_beta)
}
```

### Adding Stopping Criteria

Next, we add a stopping criteria for the algorithm. It will stop when either a certain error tolerance is reached, where an iteration is counted when all batches have been used to update the beta. So, the number of iterations here is equivalent to an epoch.

To avoid over-stepping when getting closer to the minimum, we use a decaying learning rate in the algorithm. Note that the same learning rate $\alpha$ is used to update each batch in an epoch, but this will decay as the number of iterations increases.

```{r}
gradient_descent_alg <- function(y, X, tol = 1e-06, batch_size = 10000, 
                                 init_beta = rnorm(ncol(X)), 
                                 delta = 1.2, alpha = 0.8) {
  # 1st iteration
  iter <- 1
  prev_beta <- mini_batch_gd(y, X, batch_size, init_beta, delta, alpha)
  prev_loss <- pseudo_huber_loss(y, X, prev_beta, delta)
  
  err_reduction <- 10 * tol
  while (err_reduction > tol) {
    # Next iteration of mini-batch GD
    # Use decaying learning rate
    new_beta <- mini_batch_gd(y, X, batch_size, init_beta = prev_beta, 
                              delta = delta, alpha = alpha^iter)
    new_loss <- pseudo_huber_loss(y, X, new_beta, delta)

    # Quantify the improvement
    err_reduction <- abs(prev_loss - new_loss)
  
    # Update the variables
    iter <- iter + 1
    prev_beta <- new_beta
    prev_loss <- new_loss
  }
  
  return(prev_beta)
}
```

### Cross-validation to Select the Optimal Hyperparameters

We have 2 parameters to tune:
1. Batch size
2. Learning rate $\alpha$

The optimal parameters are chosen using the k-fold cross validation based on the RMSE. Note that the algorithm is still being calculated on the pseudo-Huber loss function. The reason behind the choice of RMSE is for the comparability with other methods and it has a much lower magnitude than the corresponding loss from the pseudo-Huber loss function.

```{r}
kfold_cv <- function(y, X, k = 10, batch_size = 10000, tol = 1e-05,
                     init_beta = rnorm(ncol(X)), 
                     delta = 1.2, alpha = 0.8) {
  # Divide the dataset into k folds
  index <- ceiling(nrow(X) / k)
  
  # Empty vector to store test error for each fold
  error_vec <- c()
  
  # Cross-validation
  for (fold in 1:k) {
    start <- (fold - 1)*index + 1
    end <- min(nrow(X), start + index - 1)
    yvalid <- y[start:end]
    Xvalid <- X[start:end, ]
    
    ytrain <- y[-(start:end)]
    Xtrain <- X[-(start:end), ]
    
    # Mini-batch gradient descent and calculate test error
    beta_GD <- gradient_descent_alg(ytrain, Xtrain, tol = tol, batch_size,
                                    init_beta = init_beta, delta, alpha)
    test_error <- pseudo_huber_loss(yvalid, Xvalid, beta_GD, delta = delta)  
    error_vec <- c(error_vec, test_error)
  }
  
  # Return mean of the test error
  return(mean(error_vec))   
}
```

### Implementing the algorithm

From the EDA stage, some of the variables are highly skewed. Therefore, log-transformation is applied to them before fitting the algorithm. Note that `loudness` is being measured in decibels with negative values, so we first need to translate it such that the minimum value is 1 before applying the transformation.

In addition, the newly created variable `ngenre` is just a linear combination of all the genres columns. Hence, to avoid the issue of multicollinearity, we discard them from the model. Morever, a column of intercept is being added to the matrix of regressors.

``` {r, eval = FALSE}
# Log transform highly skewed variables
data_train$duration_ms <- log(data_train$duration_ms + 1)
data_train$loudness <- log(data_train$loudness - min(data_train$loudness) + 1)
data_train$speechiness <- log(data_train$speechiness + 1)
data_train$acousticness <- log(data_train$acousticness + 1)
data_train$instrumentalness <- log(data_train$instrumentalness + 1)
data_train$liveness <- log(data_train$liveness + 1)

y <- data_train$popularity
X <- as.matrix(data_train[,-c(1, 28)])  # ngenre is just a linear combination of other columns

# Add the intercept
X <- cbind(rep(1, length(y)), X)
```

``` {r, echo = FALSE, eval = FALSE}
write.csv(y, "y_gd.csv", row.names = FALSE)
write.csv(X, "X_gd.csv", row.names = FALSE)
```

``` {r, echo = FALSE}
y <- read.csv("y_gd.csv", header = TRUE)
y <- as.vector(y)$x
X <- as.matrix(read.csv("X_gd.csv", header = TRUE))
```

Finally, we are able to implement the algorithm across a grid of parameters and visualise the results. 

``` {r}
# Grid of parameters
batch_size <- seq(1000, 2000, by = 500)
alpha <- seq(0.05, 0.3, by = 0.05)
delta <- 5

# Empty matrix to store data
result <- array(dim = c(length(batch_size), length(alpha)),
                dimnames = list(batch_size, alpha))
```

As the starting point, we initialise the algorithm with the coefficients of a usual linear regression model (minimising RSS). This is because we expect the optimal vector of weights to not be far away from these values, hence help reducing the execution time.

``` {r, eval = FALSE}
# Initialise beta with the coefficient from lm
fit <- lm(popularity ~ ., data = data_train[,-28])
start_beta <- as.vector(coef(fit))

# By batch_size
for (i in 1:length(batch_size)) {
  # By alpha
  for (j in 1:length(alpha)) {
    result[i, j] <- kfold_cv(y, X, k = 10, 
                                  batch_size = batch_size[i], 
                                  tol = 1e-05, 
                                  init_beta = start_beta, 
                                  delta = delta, alpha = alpha[j])
  }
}
```

``` {r, echo = FALSE, warning = FALSE, eval = FALSE}
write.csv(result, "gd_result_delta_5.0.csv", row.names = TRUE, col.names = TRUE)
```

```{r, echo = FALSE}
result <- read.csv("gd_result_delta_5.0.csv", header = TRUE)
result <- result[,-1]  # discard the batch_size column
rownames(result) <- batch_size   # replace in row names
```

Then, we visualise the results by plotting the RMSE against the batch size for each learning rate $\alpha$. Given that there is an extremely high value, we discard that from the plot to zoom into the main part.

``` {r, warning = FALSE}
result <- as.data.frame(result)
result

# Discard the one with very high RMSE for visualisation purpose
ymax <- sort(as.matrix(result), decreasing = TRUE)[2]

ggplot(result) +
  geom_line(aes(x = batch_size, y = result[, 1], col = alpha[1])) + 
  geom_line(aes(x = batch_size, y = result[, 2], col = alpha[2])) +
  geom_line(aes(x = batch_size, y = result[, 3], col = alpha[3])) +
  geom_line(aes(x = batch_size, y = result[, 4], col = alpha[4])) +
  geom_line(aes(x = batch_size, y = result[, 5], col = alpha[5])) +
  ggtitle(paste("RMSE for Each Batch Size and Learning Rate with Delta = ",
                delta, sep = "")) +
  labs(x = "Batch Size", y = "RMSE", color = "Learning Rate") +
  ylim(min(result), ymax)
```

Overall, the RMSE are quite similar in magnitude, with a few outliers. Then, we select the optimal parameters ie. the pair that minimises RMSE.

```{r}
paste("Minimum Loss: ", round(min(result),4))

# Identify the index
best_size = batch_size[which(result == min(result), arr.ind = TRUE)[1]]
best_alpha = alpha[which(result == min(result), arr.ind = TRUE)[2]]

print("Optimal Parameters")
cat("Best batch size:", best_size, "\n")
cat("Best learning rate:", best_alpha, "\n")
```

### Refit the Algorithm on the Full Training Data 

```{r, eval = FALSE}
gd_beta <- gradient_descent_alg(y, X, tol = 1e-10, batch_size = best_size,
                                    init_beta = start_beta, 
                     delta = delta, alpha = best_alpha)
```

``` {r, echo = FALSE, warning = FALSE, eval = FALSE}
write.csv(gd_beta, "gd_beta.csv", row.names = FALSE, col.names = TRUE)
```

``` {r, echo = FALSE}
gd_beta <- read.csv("gd_beta.csv", header = TRUE)
gd_beta <- as.vector(gd_beta)$x
```

### Analyse the coefficients

``` {r}
beta <- data.frame(Predictors = c("Intercept", colnames(X)[-1]),
                   Beta = gd_beta)
names(beta)[2] <- "Beta"

# Sort by the highest magnitude of beta
beta <- beta[order(abs(beta$Beta), decreasing = TRUE), ]

ggplot(beta, aes(x = Predictors, y = Beta)) +
  geom_bar(stat = "identity", color = "midnightblue", fill = "midnightblue") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.9, vjust = 0.2),
        panel.grid.major = element_blank()) +
  scale_x_discrete(limits = beta$Predictors) +
  ggtitle("Slope Coefficient for Each Regressor") +
  labs(x = "Predictor", y = "Beta Coefficient")
``` 

**Comment**: The top 6 predictors, as measured by the magnitude of its corresponding slope coefficients, include 3 audio characteristics: `speechiness`, `instrumentalness` and `danceability`, and 3 genres: `classical`, `acoustic` and `r_n_b`. Afterwards, the magnitude of the coefficients are significantly lower. This agrees with the correlation plot since these variables seem to have higher correlation with popularity, despite being quite low in absolute value.

### Evaluate the Performance on the Training Set

``` {r}
# Prediction
ytrain_pred <- X %*% gd_beta

# Evaluation
residual <- y - ytrain_pred             
RMSE_gd_train <- sqrt(mean(residual^2))
TSS_train <- sum((y - mean(y))^2)
SSR_train <- sum((ytrain_pred - mean(y))^2)
RSQ_gd_train <- SSR_train / TSS_train
ntrain <- length(y)
K <- ncol(X)
adRSQ_gd_train <- 1 - (1 - RSQ_gd_train) * (ntrain-1) / (ntrain - K)

# Print the value
print("Gradient Descent: ")
cat("RMSE:", RMSE_gd_train, "\n")
cat("R-squared:", RSQ_gd_train, "\n")
cat("Adjusted R-squared:", adRSQ_gd_train, "\n")
```

**Comment**: The RMSE of 18.22 is (compare to Ruby's). It can explain about 12.3% of the variation in popularity, having adjusted for the number of predictors used. However, the performance is evaluated on the same data set that was used to train the model. Therefore, to better assess the predictive accuracy, we evaluate the model on the unseen test data.

### Evaluate the Performance on the Test Set

Firstly, we perform the same transformations to the regressors before predicting the popularity.

``` {r}
# Log transformation
data_test$duration_ms <- log(data_test$duration_ms + 1)
data_test$loudness <- log(data_test$loudness - min(data_test$loudness) + 1)
data_test$speechiness <- log(data_test$speechiness + 1)
data_test$acousticness <- log(data_test$acousticness + 1)
data_test$instrumentalness <- log(data_test$instrumentalness + 1)
data_test$liveness <- log(data_test$liveness + 1)

ytest <- data_test$popularity
Xtest <- as.matrix(data_test[,-c(1, 28)])
Xtest <- cbind(rep(1, length(ytest)), Xtest)  # Add the intercept

# Prediction
gd_pred <- Xtest %*% gd_beta
```

``` {r, echo = FALSE, eval = FALSE}
write.csv(gd_pred, "gd_pred.csv", row.names = FALSE)
```

``` {r, echo = FALSE}
gd_pred <- read.csv("gd_pred.csv", header = TRUE)
gd_pred <- as.vector(gd_pred)$V1
```

Then, we evaluate the test performance using RMSE and adjusted R-squared.

``` {r}
# Compute the metrics
RMSE_gd <- sqrt(mean((ytest - gd_pred)^2))
TSS <- sum((ytest - mean(ytest))^2)
SSR <- sum((gd_pred - mean(ytest))^2)
RSQ_gd <- SSR / TSS
n <- length(ytest)
K <- ncol(X)
adRSQ_gd <- 1 - (1 - RSQ_gd) * (n-1) / (n - K)

# Print the value
print("Gradient Descent: ")
cat("RMSE:", RMSE_gd, "\n")
cat("R-squared:", RSQ_gd, "\n")
cat("Adjusted R-squared:", adRSQ_gd, "\n")
```

**Interpretation**: RMSE is now 18.20, slightly lower than the training RMSE, which is to be expected given it is a generalisation to another unseen data. The fact that RMSE hasn't increase much suggest that the algorithm didn't overfit the data. However, the adjusted R-squared stayed relatively flat at 12.2%. (compare to Ruby's)


### [JENNY] Random forest 

# Evaluation 

